{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Camera Calibration and Fundamental Matrix Estimation with RANSAC\n",
    "(adapted from the work developed by James Hays, Cusuh Ham, Arvind Krishnakumar, Jing Wu, John Lambert, Samarth Brahmbhatt, Grady Williams, and Henry Hu, which is originally based on a similar project by Aaron Bobick.)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<font size=\"4\">The goal of this programming assignment is to introduce you to camera and scene geometry. Specifically we will estimate the **camera projection matrix**, which maps 3D world coordinates to image coordinates, as well as the **fundamental matrix**, which relates points in one scene to epipolar lines in another. The camera projection matrix and fundamental matrix can each be estimated using point correspondences. To estimate the projection matrix (camera calibration), the input is corresponding 3D and 2D points. To estimate the fundamental matrix the input is corresponding 2D points across two images. You will start out by estimating the projection matrix and the fundamental matrix for a scene with ground truth correspondences. Then you will move on to estimating the fundamental matrix using point correspondences that are obtained using SIFT.</font>\n",
    " \n",
    "<font size=\"4\">Remember these challenging images of Gaudi’s Episcopal Palace from the programming assignment 2? By using RANSAC to find the fundamental matrix with the most inliers, we can filter away spurious matches and achieve near perfect point-to-point matching as shown below:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='meta_data/gaudi.png' height='1200'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">For those of you who are passionate about **autonomous driving**, we will have a brief glimpse of the roles of epipolar geometry in this area. In practice, of course, it will be more complicated. For example, you need to deal with dynamic surroundings, different lighting condition, etc. But these fundamental geometric principles are still valuable and necessary.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "<font size=\"4\">In this programming assignment, you will work on:</font>\n",
    "* <font size=\"4\">Camera Projection Matrix  </font>\n",
    "* <font size=\"4\">Fundamental Matrix Estimation  </font>\n",
    "* <font size=\"4\">Fundamental Matrix with RANSAC   </font>\n",
    "* <font size=\"4\">Comparison of the results from previous steps</font>\n",
    "* <font size=\"4\">Using F-Matrix Estimation w/ RANSAC for Visual Odometry </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission format\n",
    "* <font size='4'>`<your_nu_username>.ipynb` (finished this file)\n",
    "    \n",
    "<font size='4'>**Note**: Do not install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything that’s not in there by default will probably cause your code to break during grading. Failure to follow any of these instructions will lead to point deductions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pa5_code.utils import (\n",
    "    verify,\n",
    "    evaluate_points,\n",
    "    visualize_points,\n",
    "    visualize_points_image,\n",
    "    plot3dview,\n",
    "    load_image,\n",
    "    draw_epipolar_lines,\n",
    "    get_matches,\n",
    "    show_correspondence2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment starts here (100 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Camera Projection Matrix (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.1: Compute the projection from $[X,Y,Z,1]$ in homogenous coordinates to $(x,y)$ in non-homogenous image coordinates. (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(P: np.ndarray, points_3d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes projection from [X,Y,Z,1] in homogenous coordinates to\n",
    "    (x,y) in non-homogenous image coordinates.\n",
    "    Args:\n",
    "        P: 3 x 4 projection matrix\n",
    "        points_3d: n x 4 array of points [X_i,Y_i,Z_i,1] in homogeneous\n",
    "            coordinates or n x 3 array of points [X_i,Y_i,Z_i]\n",
    "    Returns:\n",
    "        projected_points_2d: n x 2 array of points in non-homogenous image\n",
    "            coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    projected_points_2d = None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return projected_points_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check your implementation\n",
    "from pa5_unit_tests.test_part1 import test_projection\n",
    "print('projection():', verify(test_projection(projection)))\n",
    "# test_projection(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.2: Compute the projection matrix (20 points).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_projection_matrix(\n",
    "    points_2d: np.ndarray, points_3d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    To solve for the projection matrix. You need to set up a system of\n",
    "    equations using the corresponding 2D and 3D points.\n",
    "\n",
    "    Then you can solve this using least squares with np.linalg.lstsq() or SVD.\n",
    "    Notice you obtain 2 equations for each corresponding 2D and 3D point\n",
    "    pair. To solve this, you need at least 6 point pairs.\n",
    "\n",
    "    Args:\n",
    "        points_2d: A numpy array of shape (N, 2)\n",
    "        points_2d: A numpy array of shape (N, 3)\n",
    "\n",
    "    Returns:\n",
    "        M: A numpy array of shape (3, 4) representing the projection matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    M = None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError    \n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check your implementation\n",
    "from pa5_unit_tests.test_part1 import test_calculate_projection_matrix\n",
    "\n",
    "print('calculate_projection_matrix():', verify(\n",
    "    test_calculate_projection_matrix(calculate_projection_matrix)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see how they work in practice\n",
    "### Calculate the projection matrix given corresponding 2D & 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "img_path = '../data/CCB_GaTech/pic_a.jpg'\n",
    "points_2d = np.loadtxt('../data/CCB_GaTech/pts2d-norm-pic_a.txt')\n",
    "points_3d = np.loadtxt('../data/CCB_GaTech/pts3d-norm.txt')\n",
    "print('points2d: {}'.format(points_2d.shape))\n",
    "print('points3d: {}'.format(points_3d.shape))\n",
    "\n",
    "# (Optional) Uncomment these four lines once you have your code working with the easier, normalized points above.\n",
    "# points_2d = np.loadtxt('../data/CCB_GaTech/pts2d-pic_b.txt')\n",
    "# points_3d = np.loadtxt('../data/CCB_GaTech/pts3d.txt')\n",
    "\n",
    "M = calculate_projection_matrix(points_2d, points_3d)\n",
    "print('The projection matrix is\\n', M)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(projection, M, points_2d, points_3d);\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "plt.figure(); plt.imshow(load_image(img_path))\n",
    "visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration for Argoverse image data\n",
    "We'll now estimate the position of a camera mounted on an autonomous vehicle, using data from Argoverse. We'll use images from the \"ring front center\" camera, which faces forward.\n",
    "\n",
    "\n",
    "<img src=\"https://www.argoverse.org/assets/images/reference_images/O2V4_vehicle_annotation.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argoverse Scene 3\n",
    "img_path = '../data/argoverse_log_d60558d2_pair3/pic3.jpg'\n",
    "points_2d = np.loadtxt('../data/argoverse_log_d60558d2_pair3/points_2d.txt')\n",
    "points_3d = np.loadtxt('../data/argoverse_log_d60558d2_pair3/points_3d.txt')\n",
    "# # # Argoverse Scene 2\n",
    "# img_path = '../data/argoverse_log_d60558d2_pair2/pic2.jpg'\n",
    "# points_2d = np.loadtxt('../data/argoverse_log_d60558d2_pair2/points_2d.txt')\n",
    "# points_3d = np.loadtxt('../data/argoverse_log_d60558d2_pair2/points_3d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = calculate_projection_matrix(points_2d, points_3d)\n",
    "print('The projection matrix is\\n', M)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(projection, M, points_2d, points_3d);\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "plt.figure(); plt.imshow(load_image(img_path))\n",
    "visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these 2d-3d pairs, the \"world\" frame is defined as the \"ego-vehicle\" frame, where the origin is at the center of the back axle of the vehicle.\n",
    "\n",
    "Thus, if your camera center estimate is correct, it should tell you how far to move forward (+x) and how far to move left (+y) and move up (+z) to reach teh camera's position.\n",
    "\n",
    "\n",
    "The \"egovehicle\" coordinate system and \"camera\" coordinate system:\n",
    "<img width=\"300\"  src=\"https://user-images.githubusercontent.com/16724970/108759169-034e6180-751a-11eb-8a06-fbe344f1ee68.png\">\n",
    "<img width=\"300\" src=\"https://user-images.githubusercontent.com/16724970/108759182-06495200-751a-11eb-8162-8b17f9cdee4b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fundamental Matrix Estimation (35 points)\n",
    "<font size=\"4\">We'll now solve for the Fundamental Matrix by implementing [Hartley's 8-Point algorithm](https://www.cse.unr.edu/~bebis/CS485/Handouts/hartley.pdf). We will use the normalized version.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 2.1: Normalize the 2D coordinates (10 points).**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">As discussed in the lecture, you can use the following equation to normalize a 2D point.\n",
    "<img src='meta_data/normalize_point.png' height=10/>\n",
    "The transform matrix $T$ is the product of the scale and offset matrices. $c_u$ and $c_v$ are the mean coordinates that are to center the image data at the origin. To compute scales $s_u$ and $s_v$ you could estimate the standard deviation after subtracting the means. Then choose the scale factors $s_u$ and $s_v$ so the mean squared distance between the origin and the data points is 2 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(points: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Perform coordinate normalization through linear transformations.\n",
    "    Args:\n",
    "        points: A numpy array of shape (N, 3) representing the 2D points in\n",
    "            the image\n",
    "\n",
    "    Returns:\n",
    "        points_normalized: A numpy array of shape (N, 3) representing the\n",
    "            normalized 2D points in the image\n",
    "        T: transformation matrix representing the product of the scale and\n",
    "            offset matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    points_normalized, T = None, None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return points_normalized, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa5_unit_tests.test_part2 import test_normalize_points\n",
    "\n",
    "print('test_normalize_points():', verify(\n",
    "    test_normalize_points(normalize_points)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 2.2: Adjust the fundamental matrix to account for the normalized coordinates (5 points).**</font>\n",
    "<img src='meta_data/denormalize_F.png' height=10/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_F(\n",
    "    F_norm: np.ndarray, T_a: np.ndarray, T_b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adjusts F to account for normalized coordinates by using the transformation\n",
    "    matrices.\n",
    "\n",
    "    Args:\n",
    "        F_norm: A numpy array of shape (3, 3) representing the normalized\n",
    "            fundamental matrix\n",
    "        T_a: Transformation matrix for image A\n",
    "        T_B: Transformation matrix for image B\n",
    "\n",
    "    Returns:\n",
    "        F_orig: A numpy array of shape (3, 3) representing the original\n",
    "            fundamental matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    F_orig = None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return F_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa5_unit_tests.test_part2 import test_unnormalize_F\n",
    "\n",
    "print('test_unnormalize_F():', verify(\n",
    "    test_unnormalize_F(unnormalize_F)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 2.3: Calculate the fundamental matrix with the normalized 8-point algorithm (20 points).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_fundamental_matrix(\n",
    "    points_a: np.ndarray, points_b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the fundamental matrix. You may use the normalize_points() and\n",
    "    unnormalize_F() functions here.\n",
    "\n",
    "    Args:\n",
    "        points_a: A numpy array of shape (N, 2) representing the 2D points in\n",
    "            image A\n",
    "        points_b: A numpy array of shape (N, 2) representing the 2D points in\n",
    "            image B\n",
    "\n",
    "    Returns:\n",
    "        F: A numpy array of shape (3, 3) representing the fundamental matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    F = None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa5_unit_tests.test_part2 import test_estimate_fundamental_matrix\n",
    "\n",
    "print('test_estimate_fundamental_matrix():', verify(\n",
    "    test_estimate_fundamental_matrix(estimate_fundamental_matrix)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate fundamental matrix with real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "points_2d_pic_a = np.loadtxt('../data/CCB_GaTech/pts2d-pic_a.txt')\n",
    "points_2d_pic_b = np.loadtxt('../data/CCB_GaTech/pts2d-pic_b.txt')\n",
    "img_a = load_image('../data/CCB_GaTech/pic_a.jpg')\n",
    "img_b = load_image('../data/CCB_GaTech/pic_b.jpg')\n",
    "\n",
    "F = estimate_fundamental_matrix(points_2d_pic_a, points_2d_pic_b)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(F, img_a, img_b, points_2d_pic_a, points_2d_pic_b, figsize=(13,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fundamental Matrix with RANSAC (30 points)\n",
    "\n",
    "<font size=\"4\">**Mount Rushmore**: This pair is easy, and most of the initial matches are correct. The base fundamental matrix estimation without coordinate normalization will work fine with RANSAC. \n",
    "\n",
    "<font size=\"4\">**Notre Dame**: This pair is difficult because the keypoints are largely on the same plane. Still, even an inaccurate fundamental matrix can do a pretty good job of filtering spurious matches.  \n",
    "\n",
    "<font size=\"4\">**Gaudi**: This pair is difficult and doesn't find many correct matches unless you run at high resolution, but that will lead to tens of thousands of SIFT features, which will be somewhat slow to process. Normalizing the coordinates seems to make this pair work much better.  \n",
    "\n",
    "<font size=\"4\">**Woodruff**: This pair has a clearer relationship between the cameras (they are converging and have a wide baseline between them). The estimated fundamental matrix is less ambiguous and you should get epipolar lines qualitatively similar to part 2 of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 3.1: Calculate the number of RANSAC iterations needed for a given guarantee of\n",
    "    success (4 points).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_ransac_iterations(\n",
    "    prob_success: float, sample_size: int, ind_prob_correct: float) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the number of RANSAC iterations needed for a given guarantee of\n",
    "    success.\n",
    "\n",
    "    Args:\n",
    "        prob_success: [float] representing the desired guarantee of success\n",
    "        sample_size: [int] the number of samples included in each RANSAC\n",
    "            iteration\n",
    "        ind_prob_correct: [float] representing the probability that each element\n",
    "            in a sample is correct\n",
    "\n",
    "    Returns:\n",
    "        num_samples: int the number of RANSAC iterations needed\n",
    "\n",
    "    \"\"\"\n",
    "    num_samples = None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return int(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa5_unit_tests.test_part3 import test_calculate_num_ransac_iterations\n",
    "\n",
    "print('test_calculate_num_ransac_iterations():', verify(\n",
    "    test_calculate_num_ransac_iterations(calculate_num_ransac_iterations)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 3.2: Calculate the fundamental matrix with RANSAC. At most a single for loop can be used in your implementation (22 points).**</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_fundamental_matrix(\n",
    "    matches_a: np.ndarray, matches_b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For this section, use RANSAC to find the best fundamental matrix by\n",
    "    randomly sampling interest points. You would reuse\n",
    "    estimate_fundamental_matrix() from part 2 of this assignment and\n",
    "    calculate_num_ransac_iterations().\n",
    "\n",
    "    If you are trying to produce an uncluttered visualization of epipolar\n",
    "    lines, you may want to return no more than 30 points for either left or\n",
    "    right images.\n",
    "\n",
    "    Tips:\n",
    "        0. You will need to determine your prob_success, sample_size, and\n",
    "            ind_prob_success values. What is an acceptable rate of success? How\n",
    "            many points do you want to sample? What is your estimate of the\n",
    "            correspondence accuracy in your dataset?\n",
    "        1. A potentially useful function is numpy.random.choice for creating\n",
    "            your random samples.\n",
    "        2. You will also need to choose an error threshold to separate your\n",
    "            inliers from your outliers.\n",
    "        3. Consider the geometric distances of a keypoint to its estimated \n",
    "           epipolar line. It is a bit more robust and the error threshold\n",
    "           is easier to interpret (why?). Check the slide #70 in \n",
    "           cs5330-fall-2022-18.pptx.\n",
    "\n",
    "    Args:\n",
    "        matches_a: A numpy array of shape (N, 2) representing the coordinates\n",
    "            of possibly matching points from image A\n",
    "        matches_b: A numpy array of shape (N, 2) representing the coordinates\n",
    "            of possibly matching points from image B\n",
    "    Each row is a correspondence (e.g. row 42 of matches_a is a point that\n",
    "    corresponds to row 42 of matches_b)\n",
    "\n",
    "    Returns:\n",
    "        best_F: A numpy array of shape (3, 3) representing the best fundamental\n",
    "            matrix estimation\n",
    "        inliers_a: A numpy array of shape (M, 2) representing the subset of\n",
    "            corresponding points from image A that are inliers with respect to\n",
    "            best_F\n",
    "        inliers_b: A numpy array of shape (M, 2) representing the subset of\n",
    "            corresponding points from image B that are inliers with respect to\n",
    "            best_F\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    best_F, inliers_a, inliers_b = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return best_F, inliers_a, inliers_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check your implementation\n",
    "from pa5_unit_tests.test_part3 import test_ransac_fundamental_matrix\n",
    "\n",
    "print('test_ransac_fundamental_matrix():', verify(\n",
    "    test_ransac_fundamental_matrix(ransac_fundamental_matrix)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work with real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# Woodruff\n",
    "pic_a = load_image('../data/Woodruff_Dorm/wood1.jpg'); scale_a = 0.65\n",
    "pic_b = load_image('../data/Woodruff_Dorm/wood2.jpg'); scale_b = 0.65\n",
    "n_feat = 5e4\n",
    "\n",
    "pic_a = cv2.resize(pic_a, None, fx=scale_a, fy=scale_a)\n",
    "pic_b = cv2.resize(pic_b, None, fx=scale_b, fy=scale_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds matching points in the two images using OpenCV's implementation of SIFT.\n",
    "# There can still be many spurious matches, though.\n",
    "points_2d_pic_a, points_2d_pic_b = get_matches(pic_a, pic_b, n_feat)\n",
    "print('Found {:d} possibly matching features'.format(len(points_2d_pic_a)))\n",
    "match_image = show_correspondence2(pic_a, pic_b,\n",
    "                                   points_2d_pic_a[:, 0], points_2d_pic_a[:, 1],\n",
    "                                   points_2d_pic_b[:, 0], points_2d_pic_b[:, 1])\n",
    "plt.figure(); plt.imshow(match_image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Fundamental Matrix using RANSAC\n",
    "<font size='4' color='red'>**Task 3.3: Get accurate matches and converging epipolar lines. No need to write any code. Simply run the code below (4 points).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F, matched_points_a, matched_points_b = ransac_fundamental_matrix(points_2d_pic_a, points_2d_pic_b)\n",
    "\n",
    "# Draw the epipolar lines on the images and corresponding matches\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    matched_points_a[:, 0], matched_points_a[:, 1],\n",
    "    matched_points_b[:, 0], matched_points_b[:, 1]\n",
    ")\n",
    "plt.figure(); plt.imshow(match_image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_epipolar_lines(F, pic_a, pic_b, matched_points_a, matched_points_b, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visual Odometry (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following gif, we try to obtain the visual odometry of a camera mounted on a robot from the individual image frames.\n",
    "\n",
    "![VO](https://user-images.githubusercontent.com/16724970/100487935-34fd8b00-30d9-11eb-9941-7735fcae445c.gif \"VO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"Red\">**Task 4.1: Convert a fundamental matrix to the essential matrix (3 points).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emat_from_fmat(F: np.ndarray, K1: np.ndarray, K2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Create essential matrix from camera instrinsics and fundamental matrix\n",
    "    Args:\n",
    "        F:  A numpy array of shape (3, 3) representing the fundamental matrix between\n",
    "            two cameras\n",
    "        K1: A numpy array of shape (3, 3) representing the intrinsic matrix of the\n",
    "            first camera\n",
    "        K2: A numpy array of shape (3, 3) representing the intrinsic matrix of the\n",
    "            second camera\n",
    "\n",
    "    Returns:\n",
    "        E:  A numpy array of shape (3, 3) representing the essential matrix between\n",
    "            the two cameras.\n",
    "    \"\"\"\n",
    "    \n",
    "    E = None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check your implementation\n",
    "from pa5_unit_tests.test_part4 import test_get_emat_from_fmat\n",
    "\n",
    "print('test_get_emat_from_fmat():', verify(\n",
    "    test_get_emat_from_fmat(get_emat_from_fmat)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see check the visual odometry estimations on a real-world autonomous driving sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa5_code.vo import get_visual_odometry, plot_poses, evaluate_poses\n",
    "\n",
    "# This may take a few minutes to run across 20 image frames from the Argoverse dataset\n",
    "poses_wTi = get_visual_odometry(get_emat_from_fmat, ransac_fundamental_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red dots denote ground-truth camera poses and green ones are the estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 4.2: Get good camera pose estimation. The plotted trajectories of ground-truth and estimated camera poses should roughly match. No need to write any code. Simply run the code below (3 points).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_wTi_gt = np.load('../data/vo_seq_argoverse_273c1883/gt_poses.npy')\n",
    "plot_poses(poses_wTi, poses_wTi_gt)\n",
    "avg_r_err, avg_t_err = evaluate_poses(poses_wTi, poses_wTi_gt)\n",
    "print('Average rotation error (in degree): {}'.format(avg_r_err))\n",
    "print('Average angular error of translation (in degree): {}'.format(avg_t_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 4.3: Question: In addition to the fundamental matrix, what additional camera information is required to recover the ego-motion? (4 points)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c932221d015be5e991a60a644cf3ed71cc22fa8fa523ec551ae3ec6d94ad9811"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
